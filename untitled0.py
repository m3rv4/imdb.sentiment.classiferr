# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g0zCiuFRXy0JJ7ZgrvmhCsiR8-_LB3EN
"""

from google.colab import files
uploaded = files.upload()

import tarfile



with tarfile.open("aclImdb_v1.tar.gz", 'r:gz') as tar:
  tar.extractall(".")

import os
print(os.listdir("aclImdb"))

import os
import pandas as pd

def load_imdb_dataset(base_path):
  data = {"review": [],"label": []}

  for subset in ["train","test"]:
   for sentiment in ["pos", "neg"]:
       folder = os.path.join(base_path, subset, sentiment)

       if not os.path.isdir(folder):
          print("Folder does not exist", folder)
          continue
       files = os.listdir(folder)
       print(f" Read {len(files)}  scader from {folder}")
       for filename in files:
           file_path = os.path.join(folder, filename)

           with open(file_path, "r", encoding="utf-8") as f:
               text = f.read()
           data["review"].append(text)

           label = 1 if sentiment == "pos" else 0
           data["label"].append(label)
  df= pd.DataFrame(data)
  return df




df = load_imdb_dataset("aclImdb")
print(df.shape)
print(df.head())
print(df["label"].value_counts())

import nltk
import re
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_word = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"<.*?>", "",text)
    text = re.sub(r"[^a-zA-Z\s]","", text)
    text =" ".join([word for word in text.split() if word not in stop_word])
    return text


df["clean_review"] = df["review"].apply(clean_text)
df.head()

!wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz

!tar -xzf aclImdb_v1.tar.gz

import os

base = "aclImdb"
for set_type in ["train", "test"]:
    for label in ["pos", "neg"]:
        folder_path = os.path.join(base, set_type, label)
        print(folder_path, "â†’", os.path.isdir(folder_path),
              "num files:", len(os.listdir(folder_path)) if os.path.isdir(folder_path) else "missing")

import pandas as pd

def load_imdb_dataset(base_path):
    data = {"review": [], "label": []}

    for set_type in ["train", "test"]:
        for label in ["pos", "neg"]:
            folder_path = os.path.join(base_path, set_type, label)
            for file in os.listdir(folder_path):
                file_path = os.path.join(folder_path, file)
                with open(file_path, "r", encoding="utf-8") as f:
                    review = f.read()
                    data["review"].append(review)
                    data["label"].append(1 if label == "pos" else 0)

    return pd.DataFrame(data)

df = load_imdb_dataset("aclImdb")
print("Shape:", df.shape)
print(df.head())

print("Shape:", df.shape)
print(df.head())
print(df["label"].value_counts())

import re
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords

stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"<.*?>", " ", text)
    text = re.sub(r"[^a-z\s]", " ", text)
    words = [w for w in text.split() if w not in stop_words]
    return " ".join(words)

df["clean_review"] = df["review"].apply(clean_text)

print("Kolonat:", df.columns)
print(df[["review", "clean_review"]].head())

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df["clean_review"])
y = df["label"].values

print("TF-IDF shape:", X.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Train size:", X_train.shape, "Test size:", X_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Neg", "Pos"], yticklabels=["Neg", "Pos"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

def predict_review(review_text):
    cleaned = clean_text(review_text)
    vec = vectorizer.transform([cleaned])
    pred = model.predict(vec)[0]
    return "Positive" if pred == 1 else "Negative"

# Provo
print(predict_review("I really loved this movie, it was fantastic!"))
print(predict_review("This was the worst movie I have ever seen."))